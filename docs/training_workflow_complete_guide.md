# é€ é”™é€»è¾‘è¯¦è§£ä¸è®­ç»ƒæµç¨‹å®Œæ•´æŒ‡å—

## ä¸€ã€é€ é”™é€»è¾‘è¯¦è§£

### æ ¸å¿ƒé—®é¢˜è§£ç­”

#### Q1: ä¸€ä¸ªåŸå§‹å¥å­åªç”Ÿæˆä¸€ä¸ªé”™è¯¯å—ï¼Ÿ

**ç­”ï¼šä¸ï¼ä¸€ä¸ªå¥å­å¯ä»¥æœ‰å¤šä¸ªé”™è¯¯ã€‚**

é€ é”™æµç¨‹ï¼š
```python
1. ä»¥æ¦‚ç‡ p_corrupt (å¦‚70%) å†³å®šæ˜¯å¦é€ é”™
   - å¦‚æœéšæœºæ•° < 0.7 â†’ é€ é”™
   - å¦åˆ™ â†’ ä¿æŒåŸå¥ï¼ˆ30%ä¿ç•™æ­£ç¡®å¥å­ï¼‰

2. å¦‚æœè¦é€ é”™ï¼Œä»æ³Šæ¾åˆ†å¸ƒé‡‡æ ·é”™è¯¯æ•°é‡ n
   - lambda = 1.5 â†’ å¹³å‡1.5ä¸ªé”™è¯¯
   - èŒƒå›´ï¼š1 åˆ° max_edits (å¦‚4)
   - é‡‡æ ·ç»“æœå¯èƒ½æ˜¯ï¼š1, 2, 3, æˆ– 4 ä¸ªé”™è¯¯

3. éšæœºé€‰æ‹© n ä¸ªä¸åŒçš„ä½ç½®è¿›è¡Œé”™è¯¯æ³¨å…¥

4. å¯¹æ¯ä¸ªä½ç½®ï¼ŒæŒ‰æ¦‚ç‡é€‰æ‹©é”™è¯¯ç±»å‹ï¼š
   - 20% åˆ å­—
   - 30% é‡å¤å­—
   - 50% é”™å­—
```

**å®é™…ä¾‹å­**ï¼š
```python
åŸå¥: "è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•å¥å­"

é‡‡æ ·: n_edits = 3  # ä»æ³Šæ¾åˆ†å¸ƒé‡‡æ ·åˆ°3ä¸ªé”™è¯¯
ä½ç½®: [1, 3, 5]    # éšæœºé€‰æ‹©3ä¸ªä½ç½®
ç±»å‹: [DELETE, MULTIPLY, REPLACE]  # æ¯ä¸ªä½ç½®éšæœºé€‰æ‹©ç±»å‹

ç»“æœ: "è¿™ä¸€ä¸ªä¸ªä¸ªæµ‹éªŒå¥å­"
      â†‘   â†‘â†‘â†‘   â†‘
      åˆ   é‡å¤  é”™å­—
```

#### Q2: ä¼šæŠŠæ­£ç¡®å¥å­ä¹ŸåŠ å…¥è®­ç»ƒé›†å—ï¼Ÿ

**ç­”ï¼šä¼šï¼çº¦30%ä¿ç•™æ­£ç¡®å¥å­ã€‚**

```python
p_corrupt = 0.7  # 70%é€ é”™

# ç»“æœï¼š
# - 70% çš„å¥å­ â†’ å˜æˆé”™è¯¯å¥ï¼ˆæœ‰é”™è¯¯ï¼‰
# - 30% çš„å¥å­ â†’ ä¿æŒæ­£ç¡®ï¼ˆæ— é”™è¯¯ï¼‰

è®­ç»ƒæ•°æ®ç¤ºä¾‹ï¼š
[
    ("è¿™ä¸ªå¥å°‘å­—", "è¿™ä¸ªå¥å­å°‘å­—"),      # æœ‰é”™è¯¯
    ("æ­£ç¡®çš„å¥å­", "æ­£ç¡®çš„å¥å­"),         # æ— é”™è¯¯ï¼ˆ30%ï¼‰
    ("è¿™ä¸ªå¥å¥å­é‡å¤", "è¿™ä¸ªå¥å­é‡å¤"),  # æœ‰é”™è¯¯
    ...
]
```

**ä¸ºä»€ä¹ˆè¦ä¿ç•™æ­£ç¡®å¥å­ï¼Ÿ**
1. é¿å…è¿‡æ‹Ÿæ­£ï¼ˆæ¨¡å‹å­¦ä¼šçæ”¹ï¼‰
2. æé«˜ç²¾ç¡®ç‡ï¼ˆå‡å°‘false positiveï¼‰
3. çœŸå®åœºæ™¯ä¸­å¾ˆå¤šå¥å­æ˜¯æ­£ç¡®çš„

#### Q3: ä¸€ä¸ªå¥å­æœ€å¤šä¸€ç±»æˆ–ä¸€ä¸ªé”™è¯¯å—ï¼Ÿ

**ç­”ï¼šä¸ï¼å¯ä»¥æœ‰å¤šä¸ªé”™è¯¯ï¼Œå¤šç§ç±»å‹æ··åˆã€‚**

```python
# ç¤ºä¾‹1: å¤šä¸ªåŒç±»é”™è¯¯
åŸå¥: "è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•å¥å­"
ç»“æœ: "è¿™æ˜¯æµ‹è¯•å­"  # 2ä¸ªåˆ å­—é”™è¯¯ï¼ˆ"ä¸€ä¸ª"ã€"å¥"éƒ½åˆ äº†ï¼‰

# ç¤ºä¾‹2: å¤šç§ç±»å‹æ··åˆ
åŸå¥: "ä¸­åäººæ°‘å…±å’Œå›½å›½åŠ¡é™¢"
ç»“æœ: "ä¸­è¯äººæ°‘å…±å’Œå›½å›½å›½åŠ¡é™¢"
      â†‘             â†‘â†‘
      é”™å­—          é‡å¤å­—
# 1ä¸ªé”™å­— + 1ä¸ªé‡å¤å­—ï¼Œå…±2ä¸ªé”™è¯¯

# ç¤ºä¾‹3: å¤æ‚æƒ…å†µ
åŸå¥: "æ ¹æ®å®ªæ³•ç¬¬å…­åä¸ƒæ¡è§„å®š"
ç»“æœ: "æ ¹å®ªæ³•ç¬¬å…­ä¸ƒæ¡æ¡è§„å®š"
      â†‘     â†‘     â†‘â†‘
      åˆ     åˆ     é‡å¤
# 2ä¸ªåˆ å­— + 1ä¸ªé‡å¤å­—ï¼Œå…±3ä¸ªé”™è¯¯
```

#### Q4: åŒä¸€ä¸ªå¥å­å¯èƒ½è¢«å¼•å…¥ä¸åŒçš„é”™è¯¯å—ï¼Ÿ

**ç­”ï¼šåœ¨æ•°æ®ç”Ÿæˆæ—¶ï¼Œæ¯ä¸ªå¥å­åªç”Ÿæˆä¸€æ¬¡ï¼ˆä¸€ä¸ªé”™è¯¯ç‰ˆæœ¬ï¼‰ã€‚**

```python
# æ•°æ®ç”Ÿæˆé˜¶æ®µï¼ˆgenerate_training_data.pyï¼‰
åŸå¥: "è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•å¥å­"
  â†“
ç”Ÿæˆ1æ¬¡ â†’ ("è¿™æ˜¯æµ‹è¯•å¥å­", "è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•å¥å­")
         # åªç”Ÿæˆä¸€ä¸ªé”™è¯¯ç‰ˆæœ¬

# ä¸ä¼šç”Ÿæˆå¤šä¸ªç‰ˆæœ¬ï¼š
# âœ— ("è¿™æ˜¯æµ‹è¯•å¥å­", "è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•å¥å­")
# âœ— ("è¿™æ˜¯ä¸€ä¸€ä¸ªæµ‹è¯•å¥å­", "è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•å¥å­")
# âœ— ("è¿™æ˜°ä¸€ä¸ªæµ‹è¯•å¥å­", "è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•å¥å­")
```

**ä½†æ˜¯**ï¼Œå¦‚æœä½ æƒ³ä»åŒä¸€ä¸ªå¥å­ç”Ÿæˆå¤šä¸ªé”™è¯¯ç‰ˆæœ¬ï¼š
```python
# å¯ä»¥å¤šæ¬¡è°ƒç”¨
for _ in range(3):  # ç”Ÿæˆ3ä¸ªä¸åŒçš„é”™è¯¯ç‰ˆæœ¬
    result = augmentor.augment(sentence)
    print(result.corrupted)

# è¾“å‡ºï¼ˆæ¯æ¬¡éšæœºä¸åŒï¼‰ï¼š
# "è¿™æ˜¯æµ‹è¯•å¥å­"      # åˆ äº†"ä¸€ä¸ª"
# "è¿™æ˜¯ä¸€ä¸ªä¸ªæµ‹è¯•å¥å­"  # "ä¸ª"é‡å¤
# "è¿™æ˜°ä¸€ä¸ªæµ‹è¯•å¥å­"    # "æ˜¯"â†’"æ˜°"
```

### é€ é”™ç®—æ³•è¯¦ç»†æµç¨‹å›¾

```mermaid
graph TD
    A[è¾“å…¥: æ­£ç¡®å¥å­] --> B{éšæœºæ•° < p_corrupt?}
    
    B -->|å¦ 30%| C1[ä¿æŒåŸå¥<br/>ä¸é€ é”™]
    B -->|æ˜¯ 70%| D[ä»æ³Šæ¾åˆ†å¸ƒé‡‡æ ·<br/>é”™è¯¯æ•°é‡n]
    
    D --> E[ä»å¯ç¼–è¾‘ä½ç½®<br/>éšæœºé€‰æ‹©nä¸ªä½ç½®]
    
    E --> F[å¯¹æ¯ä¸ªä½ç½®]
    
    F --> G{é‡‡æ ·é”™è¯¯ç±»å‹}
    
    G -->|20%| H1[åˆ å­— SKIP<br/>åˆ é™¤è¯¥å­—ç¬¦]
    G -->|30%| H2[é‡å¤å­— MULTIPLY<br/>é‡å¤1-Kæ¬¡]
    G -->|50%| H3[é”™å­— REPLACE<br/>ä»æ··æ·†é›†æ›¿æ¢]
    
    H1 --> I{æ£€æŸ¥ä¿æŠ¤çº¦æŸ}
    H2 --> I
    H3 --> I
    
    I -->|ä¿æŠ¤ä½ç½®| J[è·³è¿‡è¯¥é”™è¯¯]
    I -->|å¯ä¿®æ”¹| K[åº”ç”¨é”™è¯¯]
    
    J --> L{è¿˜æœ‰ä½ç½®?}
    K --> L
    
    L -->|æ˜¯| F
    L -->|å¦| M[ç”Ÿæˆé”™è¯¯å¥å­]
    
    C1 --> N[è¾“å‡º: é”™è¯¯-æ­£ç¡®å¥å¯¹]
    M --> N
    
    style B fill:#ffe1e1
    style G fill:#fff4e1
    style I fill:#e1f5ff
```

### é€ é”™ç¤ºä¾‹æ¼”ç¤º

```python
# é…ç½®å‚æ•°
config = AugmentationConfig(
    p_corrupt=0.7,      # 70%é€ é”™
    lambda_=1.5,        # å¹³å‡1.5ä¸ªé”™è¯¯
    pi_skip=0.2,        # 20%åˆ å­—
    pi_multiply=0.3,    # 30%é‡å¤å­—
    pi_replace=0.5,     # 50%é”™å­—
    max_edits=4,        # æœ€å¤š4ä¸ªé”™è¯¯
    max_insert_k=3      # é‡å¤æœ€å¤š3æ¬¡
)

# å®é™…è¿è¡Œ100ä¸ªå¥å­çš„ç»Ÿè®¡
åŸå¥æ•°é‡: 100
ä¿ç•™æ­£ç¡®: 28 ä¸ª (28%)
é€ é”™å¥å­: 72 ä¸ª (72%)

é”™è¯¯æ•°é‡åˆ†å¸ƒ:
- 1ä¸ªé”™è¯¯: 35 å¥
- 2ä¸ªé”™è¯¯: 23 å¥
- 3ä¸ªé”™è¯¯: 11 å¥
- 4ä¸ªé”™è¯¯: 3 å¥

é”™è¯¯ç±»å‹åˆ†å¸ƒ:
- åˆ å­—: 31 ä¸ª
- é‡å¤å­—: 44 ä¸ª
- é”™å­—: 78 ä¸ª
æ€»è®¡: 153 ä¸ªé”™è¯¯ï¼ˆå¹³å‡æ¯å¥2.1ä¸ªï¼‰
```

---

## äºŒã€å¦‚ä½•ä½¿ç”¨å·²æ ‡æ³¨æ•°æ®ç«‹å³è®­ç»ƒï¼ˆå·¥ä½œæµAï¼‰

### æœ€ç®€å•çš„è®­ç»ƒæ–¹å¼

**å‰æ**ï¼šä½ æœ‰ MuCGEC/SIGHAN ç­‰æ ‡æ³¨æ•°æ®

#### æ­¥éª¤1: å‡†å¤‡æ•°æ®æ ¼å¼

æ•°æ®æ ¼å¼ï¼ˆjsonlï¼‰ï¼š
```json
{"source": "è¿™ä¸ªå¥å­æœ‰é—®æ", "target": "è¿™ä¸ªå¥å­æœ‰é—®é¢˜"}
{"source": "æ ¹æ®å®ªæ³•ç¬¬å…­ä¸ƒæ¡è§„å®š", "target": "æ ¹æ®å®ªæ³•ç¬¬å…­åä¸ƒæ¡è§„å®š"}
{"source": "æ­£ç¡®çš„å¥å­", "target": "æ­£ç¡®çš„å¥å­"}
```

æˆ–è€…ï¼ˆtsvï¼‰ï¼š
```
è¿™ä¸ªå¥å­æœ‰é—®æ\tè¿™ä¸ªå¥å­æœ‰é—®é¢˜
æ ¹æ®å®ªæ³•ç¬¬å…­ä¸ƒæ¡è§„å®š\tæ ¹æ®å®ªæ³•ç¬¬å…­åä¸ƒæ¡è§„å®š
æ­£ç¡®çš„å¥å­\tæ­£ç¡®çš„å¥å­
```

#### æ­¥éª¤2: ç›´æ¥è®­ç»ƒï¼ˆå•å¡ï¼‰

```bash
python scripts/train.py \
    --train_file ./data/mucgec_train.json \
    --dev_file ./data/mucgec_dev.json \
    --test_file ./data/mucgec_test.json \
    --data_format mucgec \
    --pretrained_model hfl/chinese-macbert-base \
    --max_seq_length 128 \
    --max_insert_num 3 \
    --batch_size 32 \
    --num_epochs 10 \
    --learning_rate 2e-5 \
    --output_dir ./outputs/exp1 \
    --experiment_name mucgec_training
```

#### æ­¥éª¤3: å¤šå¡è®­ç»ƒï¼ˆæ¨èï¼‰

```bash
# ä½¿ç”¨DDPå¤šå¡è®­ç»ƒ
bash scripts/run_ddp.sh
```

æˆ–è€…æ‰‹åŠ¨ï¼š
```bash
python -m torch.distributed.launch \
    --nproc_per_node=4 \
    --master_port=29500 \
    scripts/train.py \
    --train_file ./data/mucgec_train.json \
    --dev_file ./data/mucgec_dev.json \
    --data_format mucgec \
    --batch_size 64 \
    --num_epochs 10 \
    --output_dir ./outputs/exp1 \
    --use_ddp
```

### è®­ç»ƒæµç¨‹è¯¦è§£

```mermaid
graph TD
    A[æ ‡æ³¨æ•°æ®æ–‡ä»¶<br/>train.json] --> B[æ•°æ®åŠ è½½å™¨<br/>GapReLMDataset]
    
    B --> C1[è¯»å–é”™è¯¯-æ­£ç¡®å¥å¯¹]
    C1 --> C2[é¢„å¤„ç†<br/>å½’ä¸€åŒ–/é•¿å¥åˆ‡åˆ†]
    C2 --> C3[å­—ç¬¦å¯¹é½<br/>Levenshtein]
    
    C3 --> D[ç”Ÿæˆç›‘ç£æ ‡ç­¾]
    D --> D1[op_labels<br/>KEEP/DELETE/REPLACE]
    D --> D2[insert_labels<br/>æ’å…¥æ•°é‡0-K]
    D --> D3[Gold Template<br/>MASKä½ç½®]
    
    D1 --> E[Tokenization]
    D2 --> E
    D3 --> E
    
    E --> F[DataLoader<br/>æ‰¹å¤„ç†]
    
    F --> G[è®­ç»ƒå¾ªç¯]
    
    G --> H1[Encoder<br/>MacBERTç¼–ç ]
    H1 --> H2[Planner<br/>é¢„æµ‹opå’Œinsert]
    H2 --> H3[Template Builder<br/>æ„å»ºæ¨¡æ¿]
    H3 --> H4[Infiller<br/>å¡«å……MASK]
    
    H4 --> I[è®¡ç®—æŸå¤±]
    I --> I1[Planner Loss<br/>op + insertäº¤å‰ç†µ]
    I --> I2[Infiller Loss<br/>MLMäº¤å‰ç†µ]
    
    I1 --> J[åå‘ä¼ æ’­]
    I2 --> J
    J --> K[å‚æ•°æ›´æ–°]
    
    K --> L{éªŒè¯é›†è¯„ä¼°}
    L -->|æ¯ä¸ªepoch| M[ä¿å­˜æ£€æŸ¥ç‚¹]
    
    M --> N{è¾¾åˆ°æœ€å¤§epoch?}
    N -->|å¦| G
    N -->|æ˜¯| O[è®­ç»ƒå®Œæˆ]
    
    style A fill:#e1f5ff
    style D fill:#fff4e1
    style H2 fill:#ffe1e1
    style H4 fill:#e1ffe1
```

### Python API è®­ç»ƒï¼ˆç¼–ç¨‹æ–¹å¼ï¼‰

```python
import torch
from gap_relm.config import GapReLMConfig, get_config
from gap_relm.models import GapReLMModel
from gap_relm.data import create_data_loaders
from gap_relm.trainers import GapReLMTrainer

# 1. åŠ è½½é…ç½®
config = get_config("default")

# ä¿®æ”¹é…ç½®
config.data.train_file = "./data/mucgec_train.json"
config.data.dev_file = "./data/mucgec_dev.json"
config.data.data_format = "mucgec"
config.training.num_epochs = 10
config.training.batch_size = 32

# 2. åˆ›å»ºæ•°æ®åŠ è½½å™¨
train_loader, dev_loader, _, tokenizer = create_data_loaders(
    train_file=config.data.train_file,
    dev_file=config.data.dev_file,
    tokenizer_name=config.model.pretrained_model_name,
    max_seq_length=config.model.max_seq_length,
    batch_size=config.training.batch_size,
    data_format=config.data.data_format,
)

# 3. åˆ›å»ºæ¨¡å‹
model = GapReLMModel(
    config=config,
    pretrained_model_name=config.model.pretrained_model_name
)

# 4. åˆ›å»ºè®­ç»ƒå™¨
trainer = GapReLMTrainer(
    model=model,
    config=config,
    train_loader=train_loader,
    dev_loader=dev_loader,
    tokenizer=tokenizer
)

# 5. å¼€å§‹è®­ç»ƒ
trainer.train()

print("è®­ç»ƒå®Œæˆï¼")
```

### è®­ç»ƒä¸­çš„è‡ªåŠ¨å¤„ç†

è®­ç»ƒæ—¶ï¼Œæ¨¡å‹ä¼š**è‡ªåŠ¨å¤„ç†**ï¼š

1. **å­—ç¬¦å¯¹é½**ï¼šè‡ªåŠ¨æ¨æ–­é”™è¯¯ç±»å‹
```python
# ä½ ä¸éœ€è¦æ‰‹åŠ¨å¯¹é½
è¾“å…¥: {"source": "è¿™ä¸ªå¥å­å°‘äº†", "target": "è¿™ä¸ªçš„å¥å­å°‘äº†"}
è‡ªåŠ¨: Levenshteinå¯¹é½ â†’ è¯†åˆ«ä¸ºINSERTæ“ä½œ
```

2. **æ ‡ç­¾ç”Ÿæˆ**ï¼šè‡ªåŠ¨ç”Ÿæˆopå’Œinsertæ ‡ç­¾
```python
è‡ªåŠ¨ç”Ÿæˆ:
- op_labels = [0,0,0,0,0,0,0]
- insert_labels = [0,1,0,0,0,0,0]
```

3. **æ¨¡æ¿æ„å»º**ï¼šè‡ªåŠ¨æ„å»ºGold Template
```python
è‡ªåŠ¨æ„å»º:
template = "è¿™ä¸ª[MASK]å¥å­å°‘äº†"  # ç”¨äºè®­ç»ƒInfiller
```

4. **æ‰¹å¤„ç†**ï¼šè‡ªåŠ¨paddingå’ŒbatchåŒ–
```python
è‡ªåŠ¨å¤„ç†:
- input_ids: [batch, seq_len]
- attention_mask: [batch, seq_len]
- labels: [batch, seq_len]
```

### å¿«é€Ÿå¼€å§‹ç¤ºä¾‹

**å‡è®¾ä½ æœ‰MuCGECæ•°æ®**ï¼š

```bash
# ç¬¬1æ­¥ï¼šç¡®è®¤æ•°æ®æ ¼å¼æ­£ç¡®
head ./data/mucgec_train.json
# {"source": "é”™è¯¯å¥å­", "target": "æ­£ç¡®å¥å­"}

# ç¬¬2æ­¥ï¼šç›´æ¥å¼€å§‹è®­ç»ƒ
python scripts/train.py \
    --train_file ./data/mucgec_train.json \
    --dev_file ./data/mucgec_dev.json \
    --data_format mucgec \
    --batch_size 32 \
    --num_epochs 10 \
    --output_dir ./outputs

# å®Œæˆï¼ç­‰å¾…è®­ç»ƒç»“æŸ
```

**è®­ç»ƒè¾“å‡ºç¤ºä¾‹**ï¼š
```
Loading data...
Processing 10000 samples...
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10000/10000 [01:23<00:00]
Loaded 9847 samples

Training...
Epoch 1/10:
  100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 308/308 [05:42<00:00, train_loss=2.34]
  Dev F0.5: 0.543, Precision: 0.621, Recall: 0.487
  âœ“ Saved checkpoint to ./outputs/checkpoint_epoch_1

Epoch 2/10:
  100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 308/308 [05:38<00:00, train_loss=1.87]
  Dev F0.5: 0.612, Precision: 0.689, Recall: 0.551
  âœ“ New best! Saved checkpoint to ./outputs/best_model

...

Training complete!
Best epoch: 8
Best F0.5: 0.687
```

---

## ä¸‰ã€è®­ç»ƒæµç¨‹æ€»ç»“

### æ•°æ®å¤„ç†æµæ°´çº¿

```
æ ‡æ³¨æ•°æ®(json/tsv) 
  â†’ è¯»å–é”™è¯¯-æ­£ç¡®å¥å¯¹
  â†’ Levenshteinè‡ªåŠ¨å¯¹é½ï¼ˆæ¨æ–­é”™è¯¯ç±»å‹ï¼‰
  â†’ è‡ªåŠ¨ç”Ÿæˆç›‘ç£æ ‡ç­¾ï¼ˆop/insert/templateï¼‰
  â†’ Tokenization + æ‰¹å¤„ç†
  â†’ é€å…¥æ¨¡å‹è®­ç»ƒ
```

**å…³é”®ï¼šå…¨è‡ªåŠ¨ï¼ä½ åªéœ€è¦æä¾›é”™è¯¯-æ­£ç¡®å¥å¯¹ã€‚**

### å¯¹æ¯”ä¸¤ç§æ•°æ®æ¥æº

| ç‰¹æ€§ | å·²æ ‡æ³¨æ•°æ®ï¼ˆå·¥ä½œæµAï¼‰ | ç”Ÿæˆæ•°æ®ï¼ˆå·¥ä½œæµBï¼‰ |
|------|---------------------|-------------------|
| æ•°æ®å‡†å¤‡ | ç›´æ¥ä½¿ç”¨ | éœ€è¦è¿è¡Œç”Ÿæˆè„šæœ¬ |
| é”™è¯¯ç±»å‹ | çœŸå®é”™è¯¯åˆ†å¸ƒ | å¯æ§çš„é”™è¯¯åˆ†å¸ƒ |
| æ•°æ®é‡ | æœ‰é™ï¼ˆå‡ åƒ-å‡ ä¸‡ï¼‰ | æ— é™ï¼ˆåªè¦æœ‰cleanå¥å­ï¼‰ |
| è´¨é‡ | é«˜ï¼ˆäººå·¥æ ‡æ³¨ï¼‰ | ä¸­ï¼ˆè§„åˆ™ç”Ÿæˆï¼‰ |
| é€‚ç”¨åœºæ™¯ | è¯„ä¼°æ¨¡å‹æ€§èƒ½ | æ‰©å……è®­ç»ƒæ•°æ® |
| ä½¿ç”¨æ–¹å¼ | `--train_file mucgec_train.json` | å…ˆç”Ÿæˆâ†’å†è®­ç»ƒ |

**æœ€ä½³å®è·µ**ï¼š**æ··åˆä½¿ç”¨** = æ ‡æ³¨æ•°æ® + ç”Ÿæˆæ•°æ®

---

## å››ã€ç«‹å³å¼€å§‹è®­ç»ƒçš„æ£€æŸ¥æ¸…å•

### âœ… å‰ç½®æ£€æŸ¥

1. **æ•°æ®æ ¼å¼æ­£ç¡®**ï¼š
```bash
# æ£€æŸ¥æ–‡ä»¶å†…å®¹
head -n 3 ./data/train.json
# åº”è¯¥çœ‹åˆ°ï¼š
# {"source": "é”™è¯¯å¥", "target": "æ­£ç¡®å¥"}
# {"source": "...", "target": "..."}
```

2. **ä¾èµ–å·²å®‰è£…**ï¼š
```bash
pip install torch transformers python-Levenshtein tensorboard tqdm
```

3. **GPUå¯ç”¨**ï¼ˆå¯é€‰ï¼ŒCPUä¹Ÿèƒ½è®­ç»ƒï¼‰ï¼š
```bash
python -c "import torch; print(torch.cuda.is_available())"
# åº”è¯¥è¾“å‡ºï¼šTrue
```

### ğŸš€ å¼€å§‹è®­ç»ƒ

```bash
python scripts/train.py \
    --train_file ./data/your_train_file.json \
    --dev_file ./data/your_dev_file.json \
    --data_format mucgec \
    --output_dir ./outputs/my_exp
```

**å°±è¿™ä¹ˆç®€å•ï¼** ğŸ‰

---

## äº”ã€å¸¸è§é—®é¢˜

### Q: è®­ç»ƒå¾ˆæ…¢æ€ä¹ˆåŠï¼Ÿ
A: ä½¿ç”¨å¤šå¡è®­ç»ƒæˆ–å‡å°batch_size

### Q: å†…å­˜ä¸è¶³æ€ä¹ˆåŠï¼Ÿ
A: å‡å°max_seq_lengthæˆ–batch_size

### Q: å¦‚ä½•æé«˜F2åˆ†æ•°ï¼Ÿ
A: è°ƒæ•´F2ä¼˜åŒ–å‚æ•°ï¼ˆconfig.f2_optimizationï¼‰

### Q: å¯ä»¥ä¸­æ–­åç»§ç»­è®­ç»ƒå—ï¼Ÿ
A: å¯ä»¥ï¼Œä½¿ç”¨--resume_from_checkpoint

### Q: å¦‚ä½•æ··åˆä½¿ç”¨æ ‡æ³¨æ•°æ®å’Œç”Ÿæˆæ•°æ®ï¼Ÿ
A: å…ˆç”Ÿæˆæ•°æ®ï¼Œç„¶ååˆå¹¶åˆ°æ ‡æ³¨æ•°æ®æ–‡ä»¶ä¸­

```bash
# åˆå¹¶æ•°æ®æ–‡ä»¶
cat mucgec_train.json generated_train.jsonl > combined_train.json

# è®­ç»ƒ
python scripts/train.py --train_file combined_train.json ...
```
