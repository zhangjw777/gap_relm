# è®­ç»ƒé…ç½®æ€»ç»“ä¸ç¡®è®¤

## âœ… æ ¸å¿ƒç¡®è®¤ï¼šjointè®­ç»ƒä½¿ç”¨Gold Template

### ä»£ç è¯æ®é“¾

#### 1. æ•°æ®åŠ è½½æ—¶ç”ŸæˆGold Template

**æ–‡ä»¶**: [data/label_generator.py](data\label_generator.py)

```python
@dataclass
class GoldTemplate:
    """Gold Template (ç”¨äºè®­ç»ƒ Infiller)
    
    æ¨¡æ¿ç»“æ„: æ ¹æ® op å’Œ insert æ ‡ç­¾æ„å»ºçš„ token åºåˆ—
    - template_tokens: æ¨¡æ¿ token åºåˆ— (åŒ…å« [MASK] å’Œä¿ç•™çš„å­—ç¬¦)
    - gold_tokens: æ¯ä¸ª [MASK] ä½ç½®å¯¹åº”çš„æ­£ç¡® token
    - mask_positions: [MASK] åœ¨æ¨¡æ¿ä¸­çš„ä½ç½®åˆ—è¡¨
    """
    template_tokens: List[str]     # æ¨¡æ¿åºåˆ—
    gold_tokens: List[str]         # [MASK] ä½ç½®çš„æ­£ç¡®ç­”æ¡ˆ
    mask_positions: List[int]      # [MASK] çš„ä½ç½®ç´¢å¼•
    source: str
    target: str
```

Gold Templateæ˜¯åœ¨**æ•°æ®é¢„å¤„ç†**æ—¶å°±ç”Ÿæˆçš„ï¼Œä¸æ˜¯è®­ç»ƒæ—¶åŠ¨æ€ç”Ÿæˆã€‚

#### 2. Datasetè¿”å›Gold Template

**æ–‡ä»¶**: [data/dataset.py](data\dataset.py) Line ~320-350

```python
def __getitem__(self, idx: int) -> Dict[str, Any]:
    """è·å–å•ä¸ªæ ·æœ¬"""
    sample = self.samples[idx]  # ProcessedSampleåŒ…å«gold_template
    
    # æ„å»ºæ¨¡æ¿åºåˆ—
    template_text = ''.join(sample.gold_template.template_tokens)
    template_encoding = self.tokenizer(...)
    
    # æ„å»ºinfillæ ‡ç­¾ï¼ˆMASKä½ç½®çš„æ­£ç¡®ç­”æ¡ˆï¼‰
    gold_tokens = sample.gold_template.gold_tokens
    for i, pos in enumerate(mask_positions):
        if i < len(gold_tokens):
            infill_labels[pos] = tokenizer.convert_tokens_to_ids(gold_tokens[i])
    
    return {
        'input_ids': source_encoding,        # æºåºåˆ—
        'template_input_ids': template_ids,  # Gold Template â† è¿™æ˜¯é¢„ç”Ÿæˆçš„ï¼
        'infill_labels': infill_labels,      # MASKä½ç½®ç­”æ¡ˆ
        ...
    }
```

#### 3. Modelå‰å‘ä½¿ç”¨Gold Template

**æ–‡ä»¶**: [models/gap_relm.py](models\gap_relm.py) Line ~140-230

```python
def forward(
    self,
    input_ids: torch.Tensor,           # [batch, seq_len] æºåºåˆ—
    template_input_ids: torch.Tensor,  # [batch, template_len] Gold Template
    training_stage: str = "joint",
    ...
):
    # Plannerè®­ç»ƒï¼šåœ¨æºåºåˆ—ä¸Šé¢„æµ‹
    if training_stage in ["planner", "joint"]:
        planner_output = self.planner(
            hidden_states=encoder_hidden,  # ä»æºåºåˆ—ç¼–ç 
            op_labels=op_labels,
            insert_labels=insert_labels,
        )
    
    # Infillerè®­ç»ƒï¼šåœ¨Gold Templateä¸Šè®­ç»ƒ
    if training_stage in ["infiller", "joint"]:
        infiller_output = self.infiller(
            input_ids=template_input_ids,  # â† ä½¿ç”¨Gold Templateï¼Œä¸æ˜¯Planneræ„å»ºçš„ï¼
            labels=infill_labels,          # MASKä½ç½®çš„æ­£ç¡®ç­”æ¡ˆ
            ...
        )
    
    # è”åˆæŸå¤±
    total_loss = planner_loss + lambda_infill * infiller_loss
```

**å…³é”®ç‚¹**ï¼š
- `template_input_ids` æ¥è‡ªDatasetï¼Œæ˜¯**é¢„ç”Ÿæˆçš„Gold Template**
- **ä¸æ˜¯**åœ¨è®­ç»ƒæ—¶ç”¨Plannerçš„é¢„æµ‹æ„å»ºçš„æ¨¡æ¿
- Plannerå’ŒInfilleræ˜¯**ç‹¬ç«‹è®­ç»ƒ**çš„ï¼Œåªæ˜¯æŸå¤±è”åˆ

#### 4. Trainerè°ƒç”¨ç¡®è®¤

**æ–‡ä»¶**: [trainers/trainer.py](trainers\trainer.py) Line ~330-350

```python
def _train_epoch(self, epoch, training_stage="joint", ...):
    for batch in train_loader:
        # batchæ¥è‡ªDatasetï¼ŒåŒ…å«é¢„ç”Ÿæˆçš„Gold Template
        outputs = self.model(
            input_ids=batch['input_ids'],              # æºåºåˆ—
            template_input_ids=batch['template_input_ids'],  # Gold Template
            infill_labels=batch['infill_labels'],      # æ­£ç¡®ç­”æ¡ˆ
            training_stage=training_stage,              # "joint"
        )
        
        loss = outputs.total_loss  # planner_loss + infiller_loss
        loss.backward()
```

### ğŸ¯ è®­ç»ƒæµç¨‹å›¾

```
æ•°æ®æ–‡ä»¶(json)
    â†“
åŠ è½½(source, target)å¯¹
    â†“
Levenshteinå¯¹é½
    â†“
ç”Ÿæˆæ ‡ç­¾
    â”œâ”€ op_labels (KEEP/DELETE/REPLACE)
    â”œâ”€ insert_labels (0~K)
    â””â”€ Gold Template (åŒ…å«MASK)
    â†“
å­˜å…¥Dataset
    â†“
DataLoaderæ‰¹å¤„ç†
    â†“
è®­ç»ƒå¾ªç¯ï¼ˆjointé˜¶æ®µï¼‰
    â”œâ”€ Planner(source) â†’ é¢„æµ‹op + insert
    â”‚    â†“
    â”‚   planner_lossï¼ˆä¸çœŸå®op/insertæ ‡ç­¾å¯¹æ¯”ï¼‰
    â”‚
    â””â”€ Infiller(Gold Template) â†’ å¡«å……MASK
         â†“
        infiller_lossï¼ˆä¸æ­£ç¡®ç­”æ¡ˆå¯¹æ¯”ï¼‰
    â†“
total_loss = planner_loss + Î» * infiller_loss
    â†“
åå‘ä¼ æ’­
```

### ğŸ“ ä¸ºä»€ä¹ˆè¿™æ ·è®¾è®¡ï¼Ÿ

#### ä¼˜ç‚¹

1. **è®­ç»ƒç¨³å®š**ï¼šInfilleråœ¨æ­£ç¡®æ¨¡æ¿ä¸Šè®­ç»ƒï¼Œä¸å—Planneré”™è¯¯å½±å“
2. **é¿å…è¯¯å·®ä¼ æ’­**ï¼šPlannerçš„é¢„æµ‹é”™è¯¯ä¸ä¼šç´¯ç§¯åˆ°Infiller
3. **ç‹¬ç«‹ä¼˜åŒ–**ï¼šä¸¤ä¸ªä»»åŠ¡å¯ä»¥ç‹¬ç«‹æ”¶æ•›
4. **Teacher Forcing**ï¼šInfillerå­¦ä¹ åœ¨å®Œç¾æ¨¡æ¿ä¸Šå¡«å……

#### æ¨ç†æ—¶çš„å·®å¼‚

```python
# è®­ç»ƒæ—¶ï¼ˆjointï¼‰
planneré¢„æµ‹op/insert â†’ è®¡ç®—planner_loss
infillerå¡«å……Gold Template â†’ è®¡ç®—infiller_loss

# æ¨ç†æ—¶ï¼ˆpredictï¼‰
planneré¢„æµ‹op/insert â†’ æ„å»ºé¢„æµ‹æ¨¡æ¿ â†’ infillerå¡«å……é¢„æµ‹æ¨¡æ¿ â†’ è¾“å‡º
```

è¿™å°±æ˜¯ä¸ºä»€ä¹ˆéœ€è¦Stage Cï¼ˆjoint_finetuneï¼‰æ¥ç¼“è§£è®­ç»ƒ-æ¨ç†ä¸ä¸€è‡´ã€‚

---

## ğŸ“‹ DDPè®­ç»ƒé…ç½®æ€»ç»“

### å¿…å¡«å‚æ•°

```bash
--train_file <path>    # è®­ç»ƒæ•°æ®ï¼ˆå¿…é¡»ï¼‰
```

### æ¨èå‚æ•°

```bash
--dev_file <path>      # éªŒè¯æ•°æ®ï¼ˆå¼ºçƒˆæ¨èï¼‰
--num_gpus <N>         # GPUæ•°é‡ï¼ˆé»˜è®¤4ï¼‰
```

### å…³é”®é»˜è®¤å€¼

```bash
# è®­ç»ƒç­–ç•¥
TRAINING_STAGE="joint_finetune"    # è”åˆè®­ç»ƒï¼ˆPlanner + Infillerï¼‰
BATCH_SIZE=32                       # æ¯GPU batch size
NUM_EPOCHS=10                       # è®­ç»ƒè½®æ•°
LEARNING_RATE=2e-5                  # å­¦ä¹ ç‡

# æ¨¡å‹é…ç½®
PRETRAINED_MODEL="hfl/chinese-macbert-base"
MAX_SEQ_LENGTH=128
MAX_INSERT_NUM=3

# åŠŸèƒ½å¼€å…³
ENABLE_INSERT=true                  # å¯ç”¨æ’å…¥æ“ä½œ
ENABLE_DELETE=true                  # å¯ç”¨åˆ é™¤æ“ä½œ
ENABLE_AUX_MLM=true                 # å¯ç”¨è¾…åŠ©MLM
ENABLE_F2=true                      # å¯ç”¨F2ä¼˜åŒ–
USE_FP16=true                       # ä½¿ç”¨æ··åˆç²¾åº¦

# æ•°æ®æ ¼å¼
DATA_FORMAT="mucgec"                # æ”¯æŒmucgec/sighan/ecspell/custom/parallel
```

### è„šæœ¬ä½ç½®

```
scripts/
â”œâ”€â”€ run_ddp.sh          # ä¸»è®­ç»ƒè„šæœ¬ï¼ˆDDPå¤šå¡ï¼‰
â”œâ”€â”€ quick_start.sh      # å¿«é€Ÿå¯åŠ¨æ¨¡æ¿
â””â”€â”€ train.py            # è®­ç»ƒå…¥å£ç¨‹åº
```

---

## ğŸš€ ç«‹å³å¼€å§‹è®­ç»ƒ

### æ–¹æ³•1ï¼šä¿®æ”¹quick_start.shï¼ˆæœ€ç®€å•ï¼‰

```bash
# 1. ç¼–è¾‘ scripts/quick_start.sh
vim scripts/quick_start.sh

# ä¿®æ”¹è¿™å‡ è¡Œï¼š
TRAIN_FILE="./data/mucgec_train.json"  # ä½ çš„è®­ç»ƒæ–‡ä»¶
DEV_FILE="./data/mucgec_dev.json"      # ä½ çš„éªŒè¯æ–‡ä»¶
NUM_GPUS=2                              # ä½ çš„GPUæ•°é‡

# 2. è¿è¡Œ
bash scripts/quick_start.sh
```

### æ–¹æ³•2ï¼šç›´æ¥ä½¿ç”¨run_ddp.sh

```bash
bash scripts/run_ddp.sh \
    --train_file ./data/mucgec_train.json \
    --dev_file ./data/mucgec_dev.json \
    --num_gpus 2
```

### æ–¹æ³•3ï¼šPython API

```python
from gap_relm.config import GapReLMConfig, get_config
from gap_relm.models import GapReLMModel
from gap_relm.data import create_data_loaders
from gap_relm.trainers import GapReLMTrainer

# é…ç½®
config = get_config("default")
config.data.train_file = "./data/mucgec_train.json"
config.data.dev_file = "./data/mucgec_dev.json"
config.training.num_epochs = 10

# æ•°æ®
train_loader, dev_loader, _, tokenizer = create_data_loaders(
    train_file=config.data.train_file,
    dev_file=config.data.dev_file,
    ...
)

# æ¨¡å‹
model = GapReLMModel(config)

# è®­ç»ƒ
trainer = GapReLMTrainer(model, config, train_loader, dev_loader)
trainer.train()
```

---

## ğŸ“Š è®­ç»ƒç›‘æ§

### æŸ¥çœ‹æ—¥å¿—

```bash
# å®æ—¶æŸ¥çœ‹
tail -f ./outputs/training.log

# æŸ¥çœ‹TensorBoard
tensorboard --logdir=./outputs/tensorboard
```

### æŸ¥çœ‹GPUä½¿ç”¨

```bash
# å®æ—¶ç›‘æ§
watch -n 1 nvidia-smi

# æŸ¥çœ‹è¿›ç¨‹
nvidia-smi pmon -i 0,1,2,3
```

---

## â“ å¸¸è§é—®é¢˜

### Q: jointè®­ç»ƒæ—¶Infillerç”¨çš„æ˜¯ä»€ä¹ˆæ¨¡æ¿ï¼Ÿ

**A: Gold Templateï¼ˆé¢„ç”Ÿæˆçš„æ­£ç¡®æ¨¡æ¿ï¼‰ï¼Œä¸æ˜¯Planneré¢„æµ‹çš„æ¨¡æ¿ã€‚**

è¯¦è§æœ¬æ–‡æ¡£"æ ¸å¿ƒç¡®è®¤"éƒ¨åˆ†çš„ä»£ç è¯æ®é“¾ã€‚

### Q: é‚£æ¨ç†æ—¶å‘¢ï¼Ÿ

**A: æ¨ç†æ—¶æ‰ç”¨Planneré¢„æµ‹çš„æ¨¡æ¿ã€‚**

```python
# è®­ç»ƒ
infiller(Gold Template) â†’ ä¸æ­£ç¡®ç­”æ¡ˆå¯¹æ¯” â†’ loss

# æ¨ç†
planneré¢„æµ‹ â†’ æ„å»ºæ¨¡æ¿ â†’ infillerå¡«å…… â†’ è¾“å‡º
```

### Q: ä¸ºä»€ä¹ˆä¸åœ¨è®­ç»ƒæ—¶ä¹Ÿç”¨Planneré¢„æµ‹çš„æ¨¡æ¿ï¼Ÿ

**A: é¿å…è¯¯å·®ç´¯ç§¯ã€‚**

å¦‚æœè®­ç»ƒæ—¶ä¹Ÿç”¨Planneré¢„æµ‹çš„æ¨¡æ¿ï¼š
- Planneré¢„æµ‹é”™è¯¯ â†’ æ¨¡æ¿é”™è¯¯ â†’ Infillerå­¦åˆ°é”™è¯¯æ˜ å°„
- è¯¯å·®ç´¯ç§¯ â†’ è®­ç»ƒä¸ç¨³å®š

ä½¿ç”¨Gold Templateï¼š
- Plannerç‹¬ç«‹å­¦ä¹ é¢„æµ‹
- Infillerç‹¬ç«‹å­¦ä¹ å¡«å……
- ä¸¤ä¸ªä»»åŠ¡äº’ç›¸ä¿ƒè¿›ï¼Œä¸äº’ç›¸å¹²æ‰°

### Q: å¦‚ä½•æŸ¥çœ‹å½“å‰é…ç½®ï¼Ÿ

è¿è¡Œè®­ç»ƒè„šæœ¬æ—¶ä¼šæ‰“å°ï¼š

```
==========================================
  Gap-ReLM DDP å¤šå¡è”åˆè®­ç»ƒ
==========================================

ã€æ•°æ®é…ç½®ã€‘
  è®­ç»ƒæ–‡ä»¶: ./data/mucgec_train.json
  éªŒè¯æ–‡ä»¶: ./data/mucgec_dev.json
  æ•°æ®æ ¼å¼: mucgec

ã€è®­ç»ƒé…ç½®ã€‘
  è®­ç»ƒé˜¶æ®µ: joint_finetune
  GPUæ•°é‡:  4
  Batch Size: 32 (per GPU)
  ...
```

---

## ğŸ“š ç›¸å…³æ–‡æ¡£

- [docs/ddp_training_guide.md](ddp_training_guide.md) - DDPè®­ç»ƒè¯¦ç»†æŒ‡å—
- [docs/training_workflow_complete_guide.md](training_workflow_complete_guide.md) - å®Œæ•´è®­ç»ƒæµç¨‹
- [docs/data_processing_guide.md](data_processing_guide.md) - æ•°æ®å¤„ç†æŒ‡å—
- [README.md](../README.md) - é¡¹ç›®æ€»è§ˆ

---

## ğŸ‰ æ€»ç»“

### âœ… å·²ç¡®è®¤

1. **jointè®­ç»ƒä½¿ç”¨Gold Template** - ä»£ç è¯æ®å……åˆ†
2. **Plannerå’ŒInfillerç‹¬ç«‹è®­ç»ƒ** - é¿å…è¯¯å·®ç´¯ç§¯
3. **è®­ç»ƒé…ç½®å·²ä¼˜åŒ–** - é»˜è®¤å€¼åˆç†
4. **DDPè„šæœ¬ready** - å¯ç«‹å³ä½¿ç”¨

### ğŸš€ å¿«é€Ÿå¼€å§‹

```bash
# åªéœ€3æ­¥
bash scripts/quick_start.sh
```

æˆ–

```bash
bash scripts/run_ddp.sh \
    --train_file <ä½ çš„è®­ç»ƒæ–‡ä»¶> \
    --dev_file <ä½ çš„éªŒè¯æ–‡ä»¶> \
    --num_gpus <ä½ çš„GPUæ•°é‡>
```

**ç¥è®­ç»ƒé¡ºåˆ©ï¼** ğŸŠ
