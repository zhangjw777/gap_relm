# DDPå¤šå¡è®­ç»ƒä½¿ç”¨æŒ‡å—

## å…³é”®ç¡®è®¤ï¼šjointè®­ç»ƒæ—¶Infillerä½¿ç”¨Gold Template

### âœ… å·²ç¡®è®¤çš„è®­ç»ƒé€»è¾‘

åœ¨ `joint_finetune` é˜¶æ®µï¼ˆPlanner + Infillerå¤šä»»åŠ¡è”åˆè®­ç»ƒï¼‰ï¼š

1. **Plannerè®­ç»ƒ**ï¼šä½¿ç”¨æºåºåˆ—ï¼Œé¢„æµ‹op_labelså’Œinsert_labels
2. **Infillerè®­ç»ƒ**ï¼šä½¿ç”¨**Gold Template**ï¼ˆä»æ ‡æ³¨æ•°æ®ç”Ÿæˆçš„æ­£ç¡®æ¨¡æ¿ï¼‰ï¼Œè€Œä¸æ˜¯Planneré¢„æµ‹çš„æ¨¡æ¿

**ä»£ç ç¡®è®¤**ï¼ˆ[models/gap_relm.py](models\gap_relm.py) Line 150-230ï¼‰ï¼š

```python
def forward(
    self,
    input_ids,           # æºåºåˆ—
    template_input_ids,  # Gold Templateï¼ˆä»æ•°æ®é›†é¢„ç”Ÿæˆï¼‰
    training_stage="joint",
    ...
):
    # Plannerå‰å‘ï¼šåœ¨æºåºåˆ—ä¸Šé¢„æµ‹
    if training_stage in ["planner", "joint"]:
        planner_output = self.planner(encoder_hidden, ...)
    
    # Infillerå‰å‘ï¼šä½¿ç”¨Gold Templateï¼ˆä¸æ˜¯Planneræ„å»ºçš„ï¼‰
    if training_stage in ["infiller", "joint"]:
        infiller_output = self.infiller(
            input_ids=template_input_ids,  # â† è¿™æ˜¯Gold Templateï¼
            labels=infill_labels,           # æ­£ç¡®çš„å¡«å……æ ‡ç­¾
            ...
        )
    
    # è”åˆæŸå¤±
    total_loss = planner_loss + lambda_infill * infiller_loss
```

**æ•°æ®æµç¡®è®¤**ï¼ˆ[data/dataset.py](data\dataset.py) Line 300-400ï¼‰ï¼š

```python
def __getitem__(self, idx):
    sample = self.samples[idx]
    
    # ä»å¯¹é½ç»“æœç”ŸæˆGold Template
    template_tokens = sample.gold_template.template_tokens  # åŒ…å«MASK
    gold_tokens = sample.gold_template.gold_tokens          # MASKçš„ç­”æ¡ˆ
    
    # è¿”å›æ•°æ®
    return {
        'input_ids': source_ids,              # æºåºåˆ—
        'template_input_ids': template_ids,   # Gold Templateï¼ˆé¢„ç”Ÿæˆï¼‰
        'infill_labels': infill_labels,       # MASKä½ç½®çš„æ­£ç¡®ç­”æ¡ˆ
        ...
    }
```

### ğŸ¯ è®­ç»ƒç­–ç•¥è¯´æ˜

#### è®­ç»ƒé˜¶æ®µå¯¹æ¯”

| é˜¶æ®µ | Planner | Infiller | Templateæ¥æº |
|------|---------|----------|------------|
| Stage A: infiller_pretrain | âŒ å†»ç»“ | âœ… è®­ç»ƒ | Gold Template |
| Stage B: planner_train | âœ… è®­ç»ƒ | âŒ å¯é€‰å†»ç»“ | Gold Template |
| Stage C: joint_finetune | âœ… è®­ç»ƒ | âœ… è®­ç»ƒ | **Gold Template** |

**å…³é”®ç‚¹**ï¼šæ‰€æœ‰é˜¶æ®µéƒ½ä½¿ç”¨Gold Templateè®­ç»ƒInfillerï¼Œé¿å…è®­ç»ƒ-æ¨ç†ä¸ä¸€è‡´ã€‚

#### joint_finetuneçš„ä¼˜åŠ¿

1. **Plannerå­¦ä¹ é¢„æµ‹**ï¼šopå’Œinsertæ ‡ç­¾
2. **Infillerå­¦ä¹ å¡«å……**ï¼šåœ¨æ­£ç¡®çš„æ¨¡æ¿ä¸Šå¡«å……MASK
3. **è”åˆä¼˜åŒ–**ï¼šä¸¤ä¸ªä»»åŠ¡äº’ç›¸ä¿ƒè¿›
4. **é¿å…è¯¯å·®ç´¯ç§¯**ï¼šè®­ç»ƒæ—¶ä¸ç”¨Plannerçš„é”™è¯¯é¢„æµ‹

---

## å¿«é€Ÿå¼€å§‹ï¼šå¤šå¡è”åˆè®­ç»ƒ

### æ–¹å¼1ï¼šä½¿ç”¨é»˜è®¤é…ç½®

```bash
# æœ€ç®€å•çš„æ–¹å¼
bash scripts/run_ddp.sh \
    --train_file ./data/mucgec_train.json \
    --dev_file ./data/mucgec_dev.json \
    --num_gpus 2
```

### æ–¹å¼2ï¼šè‡ªå®šä¹‰é…ç½®

```bash
bash scripts/run_ddp.sh \
    --train_file ./data/mucgec_train.json \
    --dev_file ./data/mucgec_dev.json \
    --num_gpus 2 \
    --batch_size 32 \
    --num_epochs 10 \
    --learning_rate 2e-5 \
    --output_dir ./outputs/exp1 \
    --experiment_name mucgec_joint_training
```

### æ–¹å¼3ï¼šä¿®æ”¹è„šæœ¬ä¸­çš„é»˜è®¤å‚æ•°

ç¼–è¾‘ `scripts/run_ddp.sh`ï¼Œä¿®æ”¹ä»¥ä¸‹å‚æ•°ï¼š

```bash
# å¿…å¡«å‚æ•°ï¼ˆè¿è¡Œæ—¶æä¾›ï¼‰
TRAIN_FILE=""                              # é€šè¿‡å‘½ä»¤è¡ŒæŒ‡å®š

# åŸºç¡€é…ç½®ï¼ˆå¯åœ¨è„šæœ¬ä¸­ä¿®æ”¹é»˜è®¤å€¼ï¼‰
NUM_GPUS=2                                 # ä½ çš„GPUæ•°é‡
BATCH_SIZE=32                              # æ¯ä¸ªGPUçš„batch size
NUM_EPOCHS=10                              # è®­ç»ƒè½®æ•°
LEARNING_RATE=2e-5                         # å­¦ä¹ ç‡

# è®­ç»ƒç­–ç•¥
TRAINING_STAGE="joint_finetune"            # è”åˆè®­ç»ƒï¼ˆæ¨èï¼‰
```

ç„¶åè¿è¡Œï¼š
```bash
bash scripts/run_ddp.sh \
    --train_file ./data/mucgec_train.json \
    --dev_file ./data/mucgec_dev.json
```

---

## å¿…å¡«å‚æ•°è¯´æ˜

### ğŸ”´ å¿…é¡»æä¾›çš„å‚æ•°

1. **--train_file**ï¼šè®­ç»ƒæ•°æ®æ–‡ä»¶è·¯å¾„
   ```bash
   --train_file ./data/mucgec_train.json
   ```
   
   æ ¼å¼è¦æ±‚ï¼š
   ```json
   {"source": "é”™è¯¯å¥", "target": "æ­£ç¡®å¥"}
   {"source": "é”™è¯¯å¥", "target": "æ­£ç¡®å¥"}
   ```

### ğŸŸ¡ å¼ºçƒˆæ¨èæä¾›çš„å‚æ•°

2. **--dev_file**ï¼šéªŒè¯æ•°æ®æ–‡ä»¶è·¯å¾„
   ```bash
   --dev_file ./data/mucgec_dev.json
   ```
   - ç”¨äºæ¯ä¸ªepochåè¯„ä¼°æ¨¡å‹
   - ç”¨äºä¿å­˜æœ€ä½³æ¨¡å‹
   - å¦‚æœä¸æä¾›ï¼Œåªè¿›è¡Œè®­ç»ƒä¸è¯„ä¼°

3. **--num_gpus**ï¼šGPUæ•°é‡
   ```bash
   --num_gpus 2  # æ ¹æ®ä½ çš„å®é™…GPUæ•°é‡
   ```
   - é»˜è®¤æ˜¯4ï¼Œæ ¹æ®å®é™…æƒ…å†µä¿®æ”¹
   - æŸ¥çœ‹GPU: `nvidia-smi`

### ğŸŸ¢ å¯é€‰å‚æ•°ï¼ˆæœ‰é»˜è®¤å€¼ï¼‰

å…¶ä»–å‚æ•°éƒ½æœ‰åˆç†çš„é»˜è®¤å€¼ï¼Œå¯ä»¥ä¸æŒ‡å®šï¼š

```bash
--data_format mucgec              # æ•°æ®æ ¼å¼
--batch_size 32                   # batch size
--num_epochs 10                   # è®­ç»ƒè½®æ•°
--learning_rate 2e-5              # å­¦ä¹ ç‡
--output_dir ./outputs            # è¾“å‡ºç›®å½•
--experiment_name gap_relm        # å®éªŒåç§°
```

---

## å®Œæ•´è®­ç»ƒæµç¨‹ç¤ºä¾‹

### ç¤ºä¾‹1ï¼šä½¿ç”¨MuCGECæ•°æ®è®­ç»ƒ

```bash
# 1. å‡†å¤‡æ•°æ®
ls ./data/mucgec_train.json
ls ./data/mucgec_dev.json

# 2. å¼€å§‹è®­ç»ƒï¼ˆ4å¡ï¼‰
bash scripts/run_ddp.sh \
    --train_file ./data/mucgec_train.json \
    --dev_file ./data/mucgec_dev.json \
    --num_gpus 2 \
    --batch_size 32 \
    --num_epochs 10 \
    --output_dir ./outputs/mucgec_exp1

# 3. æŸ¥çœ‹è®­ç»ƒæ—¥å¿—
tensorboard --logdir=./outputs/mucgec_exp1/tensorboard
```

### ç¤ºä¾‹2ï¼šä½¿ç”¨SIGHANæ•°æ®è®­ç»ƒ

```bash
bash scripts/run_ddp.sh \
    --train_file ./data/sighan_train.tsv \
    --dev_file ./data/sighan_dev.tsv \
    --data_format sighan \
    --num_gpus 2 \
    --output_dir ./outputs/sighan_exp1
```

### ç¤ºä¾‹3ï¼šä½¿ç”¨ç”Ÿæˆçš„æ•°æ®è®­ç»ƒ

```bash
# å…ˆç”Ÿæˆæ•°æ®
python scripts/generate_training_data.py

# ç„¶åè®­ç»ƒ
bash scripts/run_ddp.sh \
    --train_file ./generated_data/train.jsonl \
    --dev_file ./generated_data/dev.jsonl \
    --num_gpus 2
```

---

## è®­ç»ƒè¿‡ç¨‹ä¸­çš„è¾“å‡º

### å¯åŠ¨æ—¶çš„è¾“å‡º

```
==========================================
  Gap-ReLM DDP å¤šå¡è”åˆè®­ç»ƒ
==========================================

ã€æ•°æ®é…ç½®ã€‘
  è®­ç»ƒæ–‡ä»¶: ./data/mucgec_train.json
  éªŒè¯æ–‡ä»¶: ./data/mucgec_dev.json
  æ•°æ®æ ¼å¼: mucgec

ã€è®­ç»ƒé…ç½®ã€‘
  è®­ç»ƒé˜¶æ®µ: joint_finetune
  GPUæ•°é‡:  4
  Batch Size: 32 (per GPU)
  æ€»Batch: 128
  è®­ç»ƒè½®æ•°: 10
  å­¦ä¹ ç‡:   2e-5

ã€æ¨¡å‹é…ç½®ã€‘
  é¢„è®­ç»ƒæ¨¡å‹: hfl/chinese-macbert-base
  æœ€å¤§åºåˆ—é•¿åº¦: 128
  æœ€å¤§æ’å…¥æ•°: 3

ã€è¾“å‡ºé…ç½®ã€‘
  è¾“å‡ºç›®å½•: ./outputs
  å®éªŒåç§°: gap_relm_joint_training

ã€åŠŸèƒ½å¼€å…³ã€‘
  å¯ç”¨æ’å…¥: true
  å¯ç”¨åˆ é™¤: true
  è¾…åŠ©MLM:  true
  F2ä¼˜åŒ–:   true
  FP16:     true

==========================================

ğŸš€ Starting training...
```

### è®­ç»ƒä¸­çš„è¾“å‡º

```
Loading data...
Processing 10000 samples...
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10000/10000 [01:23<00:00]
Loaded 9847 samples

Starting Gap-ReLM Training
Experiment: gap_relm_joint_training
Device: cuda
Distributed: True
World size: 4
==========================================

Epoch 0 [joint]:
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 77/77 [05:42<00:00, loss=2.34, lr=1.2e-05]

Evaluating:
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:15<00:00]

Validation metrics:
  total_loss: 2.156
  planner_loss: 1.234
  infill_loss: 0.922
âœ“ New best! Saved checkpoint to ./outputs/best_stage_c

Epoch 1 [joint]:
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 77/77 [05:38<00:00, loss=1.87, lr=1.8e-05]
...
```

### å®Œæˆåçš„è¾“å‡º

```
==========================================
  âœ… Training completed successfully!
==========================================

ã€è¾“å‡ºç›®å½•ã€‘
  æ¨¡å‹æ£€æŸ¥ç‚¹: ./outputs/
  TensorBoard: tensorboard --logdir=./outputs/tensorboard
```

---

## è®­ç»ƒåçš„æ–‡ä»¶ç»“æ„

```
./outputs/
â”œâ”€â”€ best_stage_c/              # æœ€ä½³æ¨¡å‹
â”‚   â”œâ”€â”€ pytorch_model.bin
â”‚   â”œâ”€â”€ config.json
â”‚   â””â”€â”€ training_args.json
â”œâ”€â”€ checkpoint-1000/           # å®šæœŸæ£€æŸ¥ç‚¹
â”œâ”€â”€ checkpoint-2000/
â”œâ”€â”€ tensorboard/               # TensorBoardæ—¥å¿—
â”‚   â””â”€â”€ events.out.tfevents...
â””â”€â”€ training.log               # è®­ç»ƒæ—¥å¿—
```

---

## å¸¸è§é—®é¢˜

### Q1: å¦‚ä½•ç¡®è®¤ä½¿ç”¨äº†å‡ å¼ GPUï¼Ÿ

```bash
# è®­ç»ƒå‰æŸ¥çœ‹GPU
nvidia-smi

# è®­ç»ƒæ—¶æŸ¥çœ‹GPUä½¿ç”¨æƒ…å†µ
watch -n 1 nvidia-smi
```

### Q2: å†…å­˜ä¸è¶³æ€ä¹ˆåŠï¼Ÿ

```bash
# æ–¹æ³•1: å‡å°batch size
--batch_size 16  # æˆ–æ›´å°

# æ–¹æ³•2: å‡å°åºåˆ—é•¿åº¦
--max_seq_length 64

# æ–¹æ³•3: ä½¿ç”¨æ¢¯åº¦ç´¯ç§¯
--gradient_accumulation_steps 2
```

ä¿®æ”¹è„šæœ¬ä¸­çš„å‚æ•°ï¼š
```bash
BATCH_SIZE=16                  # å‡å°
GRADIENT_ACCUMULATION_STEPS=2  # å¢å¤§ï¼ˆç­‰æ•ˆbatch size = 16*2*4 = 128ï¼‰
```

### Q3: å¦‚ä½•æ¢å¤è®­ç»ƒï¼Ÿ

```bash
python scripts/train.py \
    --resume_from ./outputs/checkpoint-1000 \
    ...å…¶ä»–å‚æ•°...
```

### Q4: å¦‚ä½•åªä½¿ç”¨éƒ¨åˆ†GPUï¼Ÿ

```bash
# æ–¹æ³•1: æŒ‡å®šå¯è§GPU
CUDA_VISIBLE_DEVICES=0,1 bash scripts/run_ddp.sh \
    --train_file ... \
    --num_gpus 2

# æ–¹æ³•2: ä¿®æ”¹è„šæœ¬
--num_gpus 2  # ä½¿ç”¨å‰2å¼ GPU
```

### Q5: è®­ç»ƒå¤ªæ…¢æ€ä¹ˆåŠï¼Ÿ

ä¼˜åŒ–ç­–ç•¥ï¼š
1. å¢åŠ batch sizeï¼ˆå¦‚æœå†…å­˜å…è®¸ï¼‰
2. ä½¿ç”¨FP16æ··åˆç²¾åº¦ï¼ˆå·²é»˜è®¤å¯ç”¨ï¼‰
3. å¢åŠ num_workersï¼ˆæ•°æ®åŠ è½½çº¿ç¨‹ï¼‰
4. ä½¿ç”¨æ›´å¿«çš„GPU
5. å¯ç”¨æ•°æ®ç¼“å­˜ï¼ˆå·²é»˜è®¤å¯ç”¨ï¼‰

---

## é«˜çº§é…ç½®

### ä¿®æ”¹F2ä¼˜åŒ–å‚æ•°

ç¼–è¾‘è„šæœ¬ä¸­çš„F2å‚æ•°ï¼š
```bash
# æé«˜å¬å›ï¼ˆF2ä¼˜åŒ–ï¼‰
DELETE_THRESHOLD=0.3  # é™ä½åˆ é™¤é˜ˆå€¼ï¼ˆæ›´æ¿€è¿›ï¼‰
INSERT_THRESHOLD=0.3  # é™ä½æ’å…¥é˜ˆå€¼ï¼ˆæ›´æ¿€è¿›ï¼‰
```

### åˆ†é˜¶æ®µè®­ç»ƒ

```bash
# Stage A: Infilleré¢„è®­ç»ƒ
TRAINING_STAGE="infiller_pretrain"

# Stage B: Plannerè®­ç»ƒ
TRAINING_STAGE="planner_train"

# Stage C: è”åˆå¾®è°ƒï¼ˆæ¨èï¼‰
TRAINING_STAGE="joint_finetune"
```

### æ¶ˆèå®éªŒ

```bash
# ç¦ç”¨æ’å…¥æ“ä½œ
ENABLE_INSERT=false

# ç¦ç”¨åˆ é™¤æ“ä½œ
ENABLE_DELETE=false

# ç¦ç”¨è¾…åŠ©MLM
ENABLE_AUX_MLM=false

# ç¦ç”¨F2ä¼˜åŒ–
ENABLE_F2=false
```

---

## æ€»ç»“

### âœ… å·²ç¡®è®¤ï¼šjointè®­ç»ƒä½¿ç”¨Gold Template

- **Planner**ï¼šåœ¨æºåºåˆ—ä¸Šé¢„æµ‹opå’Œinsert
- **Infiller**ï¼šåœ¨Gold Templateä¸Šè®­ç»ƒå¡«å……MASK
- **è”åˆæŸå¤±**ï¼šplanner_loss + lambda * infiller_loss
- **é¿å…è¯¯å·®ä¼ æ’­**ï¼šè®­ç»ƒæ—¶ä¸ç”¨Planneré¢„æµ‹çš„æ¨¡æ¿

### ğŸš€ å¼€å§‹è®­ç»ƒåªéœ€3æ­¥

```bash
# 1. å‡†å¤‡æ•°æ®
# 2. è¿è¡Œè„šæœ¬
bash scripts/run_ddp.sh \
    --train_file ./data/train.json \
    --dev_file ./data/dev.json \
    --num_gpus 2

# 3. ç­‰å¾…å®Œæˆ
```

### ğŸ“Š ç›‘æ§è®­ç»ƒ

```bash
# å®æ—¶æŸ¥çœ‹æ—¥å¿—
tail -f ./outputs/training.log

# æŸ¥çœ‹TensorBoard
tensorboard --logdir=./outputs/tensorboard

# æŸ¥çœ‹GPUä½¿ç”¨
watch -n 1 nvidia-smi
```

**ç¥è®­ç»ƒé¡ºåˆ©ï¼** ğŸ‰
